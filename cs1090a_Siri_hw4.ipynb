{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"cs1090a_hw4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS1090A Introduction to Data Science: \n",
    "## Homework 4: Classification: Predicting College Admissions\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2024**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai\n",
    "\n",
    "<hr style=\"height:2.4pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"background: lightgreen; border: thin solid black; border-radius: 2px; padding: 5px\">\n",
    "\n",
    "### Instructions\n",
    "- To submit your notebook, follow the instructions given in on the Canvas assignment page.\n",
    "- Plots should be legible and interpretable *without having to refer to the code that generated them*. They should include labels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
    "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you believe the plot *means*.\n",
    "- Autograding tests are mostly to help you debug. The tests are not exhaustive so simply passing all tests may not be sufficient for full credit.\n",
    "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
    "- Enable scrolling output on cells with slighlty longer output. Avoid excessively long output.\n",
    "- Feel free to add additional code or markdown cells as needed.\n",
    "- Don't forget to include a written response when one is requested by a question prompt.\n",
    "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells (note that this can take a few minutes). \n",
    "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# pandas tricks for better display\n",
    "pd.options.display.max_columns = 50  \n",
    "pd.options.display.max_rows = 500     \n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.precision = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculating total notebook runtime\n",
    "notebook_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook contents\n",
    "\n",
    "- [**Overview and data description**](#intro)\n",
    "\n",
    "\n",
    "- [**Question 1: Data exploration using train and basic models**](#part1)\n",
    "\n",
    "- [**Question 2: Interpretable modeling**](#part2)\n",
    "\n",
    "- [**Question 3: Harvard and Yale?**](#part3)\n",
    "\n",
    "- [**Question 4: Building predictive models for admitted**](#part4)\n",
    "\n",
    "- [**Question 5: Evaluating results**](#part5)\n",
    "\n",
    "- [**Question 6: BONUS!**](#part6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "\n",
    "## Overview and data description\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "### Predicting admissions into elite universities\n",
    "\n",
    "In this problem set we will model the chances of high school students being accepted into two different elite undergraduate colleges: Harvard and Yale.\n",
    "\n",
    "The data are provided in the file `data/college_admissions.csv` and were scraped in 2022 from [collegedata.com](https://www.collegedata.com/) (where applicants volunteer to share their information).  Each observation corresponds to an applicant to one of the two different colleges (note: the same applicant may show up in two rows: once for each college).  The main response is the `\"admitted\"` variable (1 = admitted, 0 = denied), and there are are several predictors to consider:\n",
    "\n",
    "- **id**: a unique identifier for the applicant \n",
    "- **test_score**: a standardized measurement of the applicant's highest ACT or SAT combined score (2400 is the maximum)\n",
    "- **ap**: the number of AP tests taken\n",
    "- **avg_ap**: the average score on the AP tests taken (0 if no tests were taken)\n",
    "- **sat_subjects**: the number of SAT subject tests taken\n",
    "- **gpa**: the unweighted GPA of the applicant (max of 4.0)\n",
    "- **female**:  a binary indicator for gender: 1 = female, 0 = otherwise\n",
    "- **minority**: a binary indicator for under-represented minority: 1 = minority, 0 = otherwise \n",
    "- **international**: a binary indicator for international status: 1 = international, 0 = United States\n",
    "- **sports**: a binary indicator for High School All-American: 1 = all-American athlete, 0 = otherwise\n",
    "- **harvard**: a categorical variable for school applied to: 1 = Harvard, 0 = Yale\n",
    "- **early_app**: a binary indicator for application type: 1 = early action, 0 = regular\n",
    "- **alumni**:  a binary indicator for parents' alumni status of school: 1 = a parent is an alumnus, 0 = otherwise\n",
    "- **program**: the program applied to by the student with many choices (we will not use this as a predictor)\n",
    "- **add_info**: additional (optional) info provided by applicant (we will not use this as a predictor)\n",
    "\n",
    "**The main set of 12 predictors is:**\n",
    "\n",
    "```python\n",
    "[\n",
    "    \"test_score\", \"ap\", \"avg_ap\", \"sat_subjects\", \n",
    "    \"gpa\", \"female\", \"minority\", \"international\",\n",
    "    \"sports\", \"harvard\", \"early_app\", \"alumni\",\n",
    "]\n",
    "```\n",
    "\n",
    "Please note, you may need to modify this list when fitting different models.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTES:**\n",
    "\n",
    "- Unless stated otherwise, all logistic regression models should be unregularized (use `penalty=None`) and include the intercept (which is the default in `sklearn`).\n",
    "\n",
    "\n",
    "- When printing your output (e.g. coefficients, accuracy scores, etc.), DO NOT just print numbers without context. Please be certain provide clarifying labels for all printed numbers and limit the number of digits showing after decimals to a reasonable length (e.g. 4 decimal points for coefficients and accuracy scores).\n",
    "\n",
    "\n",
    "- Also be sure to practice good data science principles: always use train to do analysis and never touch the test set until the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 1: Data exploration using train and basic models</div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "The first step is to split the observations into an approximate 80-20 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits). It also prints the dataset's shape before splitting and after splitting. \n",
    "\n",
    "**IMPORTANT:** While an argument could be made to scale our predictors here, please **DO NOT** do so **UNTIL** it is requested of you in **[Question 4.1](#part4)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "## DO NOT MODIFY THIS CODE ##\n",
    "#############################\n",
    "np.random.seed(121)\n",
    "\n",
    "college = pd.read_csv(\"data/college_admissions.csv\")\n",
    "\n",
    "college_train, college_test = train_test_split(\n",
    "    college,\n",
    "    test_size=0.2,\n",
    "    random_state=121,\n",
    "    shuffle=True,\n",
    "    stratify=college[\"admitted\"],\n",
    ")\n",
    "\n",
    "print(college.shape)\n",
    "print(college_train.shape, college_test.shape)\n",
    "display(college_train.head())\n",
    "college_train.info()\n",
    "\n",
    "# Separate predictors from response\n",
    "X_train, y_train = college_train.drop(columns=[\"admitted\"]), college_train[\"admitted\"]\n",
    "X_test, y_test = college_test.drop(columns=[\"admitted\"]), college_test[\"admitted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q1.1** Consider a baseline, \"naive\" model that predicts that *ALL* applicants belong to the same class. This class prediction is *not* determined by an applicant's feature values, but rather by the proportion of observations in the training data belonging to each class.\n",
    "\n",
    "Calculate the accuracy of this naive model on the training data and store it in `naive_train_acc`.\n",
    "    \n",
    "**NOTE:** For this assignment, use the convention that accuracies range from 0 to 1 (to be consistant with sklearn).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "naive_train_acc = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"Naive Classification Model Based on Training Proportions\")\n",
    "print(f\"\\tTrain Accuracy: {naive_train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q1.2** Let's investigate how each of our 12 predictors is associated with whether or not an applicant is admitted into the college to which they applied (`admitted`). To this end, create a separate **visual** for each of our predictors to investigate their relationship with college admissions. **Suggestion:** Place these 12 visuals in a grid of subplots with 3 columns and 4 rows.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dummy variable\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3** Based on the visuals above, which predictor seems to have the most potential for predicting `admitted`? Why do you think this it the best potential single predictor?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "    \n",
    "**Q1.4** Fit a logistic regression to predict `admitted` from `harvard` (call it `logit1_4`).  \n",
    "- Store the coefficient and intercept in `logit1_4_coef` and `logit1_4_intercept`. Interpret these values.\n",
    "- Use $\\LaTeX$ to demonstrate how you can use these coefficient values to calculate the estimated probabilities of getting into each school.\n",
    "- Store the results of the above calculation in `p_harvard` and `p_yale`. \n",
    "- Which school is easier to get into according to this model?\n",
    "\n",
    "**IMPORTANT:** \n",
    "- Remember, all models in this assignment should be **unregularized** unless you are specifically asked to use regularization for a particular model.\n",
    "- Use `random_state = 109` for all of your `LogisticRegression` and `LogisticRegressionCV` models in this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "# Fit logistic regression model without regularization\n",
    "logit1_4 = ...\n",
    "# Identify and report coefficients\n",
    "logit1_4_coef = ...\n",
    "logit1_4_intercept = ...\n",
    "# Calculate and report probabilities\n",
    "p_harvard = ...\n",
    "p_yale = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The coefficient for the 'logit1_4' model's only predictor 'harvard' \"\n",
    "    \"is {:.4f} and the intercept is {:.4f}\\n\".format(\n",
    "        logit1_4_coef, logit1_4_intercept\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Estimated probability of acceptance for Harvard: {:.4f}\"\n",
    "    .format(p_harvard)\n",
    ")\n",
    "print(\n",
    "    \"Estimated probability of acceptance for Yale: {:.4f}\"\n",
    "    .format(p_yale)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q1.5** Create and display a [contingency table](https://en.wikipedia.org/wiki/Contingency_table) between `admitted` and `harvard`.  Use this table to calculate and confirm the coefficient estimates in the `logit1_4` model (both the intercept and slope). Show this calculation using $\\LaTeX$ in a markdown cell.\n",
    "    \n",
    "**Hint:** The Pandas [crosstab](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html) method may be helpful here.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q1.6** Compare the estimated probabilities of being admitted into the schools to the overall acceptance rate (as seen [here](https://www.ivycoach.com/2022-ivy-league-admissions-statistics/)).  Why may what you've observed in this comparison be the case?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 2: Interpretable modeling</div>\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q2.1** \n",
    "    \n",
    "- Fit a logistic regression model to predict `admitted` from `test_score` alone. Call it `logit2_1`.\n",
    "- Store the learned parameters in `logit2_1_intercept` and `logit2_1_coef_test_score`. \n",
    "- Store the train accuracy and mean 10-fold cross-validation accuracy in `acc_train_logit2_1` and `acc_cv_logit2_1`.\n",
    "\n",
    "**Note:** The coefficients and train accuracies should come from a model fit on all the training data. To calculate the mean 10-fold cross-validation accuracy, use `cross_val_score`. This should be the strategy used throughout the notebook except when using `LogisticRegressionCV` in later questions (since this model already gives you CV scores \"for free\"). Will will assume 10 folds whenever we cross-validate in this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "logit2_1 = ...\n",
    "\n",
    "logit2_1_intercept = ...\n",
    "logit2_1_coef_test_score = ...\n",
    "\n",
    "acc_train_logit2_1 = ...\n",
    "acc_cv_logit2_1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"The intercept and coefficient for the 'logit2_1' model are:\")\n",
    "\n",
    "print(\"\\t{:<20}{:.4f}\".format('intercept', logit2_1_intercept))\n",
    "print(\"\\t{:<20}{:.4f}\".format('test_score', logit2_1_coef_test_score))\n",
    "\n",
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit2_1' are:\"\n",
    "    \"\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\".format(\n",
    "        acc_train_logit2_1, acc_cv_logit2_1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q2.2**\n",
    "    \n",
    "- What is the estimated probability of an applicant being admitted with a `test_score` of 2250? Store this in `prob_test_2250`.\n",
    "- What about if they had a perfect test score of 2400? Store this in `prob_test_2400`.\n",
    "- What test score would be needed to have a 50-50 chance (i.e. 0.5 probability) of being admitted? Store this in `test_50_50`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "prob_test_2250 = ...\n",
    "prob_test_2400 = ...\n",
    "\n",
    "test_50_50 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The estimated chances of an applicant being admitted with \"\n",
    "    \"the following two 'test' scores:\\n\\n\\tscore\\tprobabilities\"\n",
    "    \"\\n\\t2250\\t{:.4f}\\n\\t2400\\t{:.4f}\\n\"\n",
    "    .format(prob_test_2250, prob_test_2400)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The test score required to have a 50-50 chance of being \"\n",
    "    \"admitted is approximately:\\n\\n\\t{:.2f}\"\n",
    "    .format(test_50_50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q2.3** Fit a logistic regression model to predict `admitted` from `test_score` and `avg_ap` (call it `logit2_3`).  Store the train and mean cv accuracies calculated as before in `acc_train_logit2_3` and `acc_cv_logit2_3`. Then use the provided code to print out the coefficient estimates along with the accuracy scores.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "logit2_3 = ...\n",
    "\n",
    "logit2_3_intercept = ...\n",
    "logit2_3_coef_test_score = ...\n",
    "logit2_3_coef_avg_ap = ...\n",
    "\n",
    "acc_train_logit2_3 = ...\n",
    "acc_cv_logit2_3 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Print the results of logit2_3\n",
    "print(\"The intercept and coefficients for the 'logit2_3' model are:\")\n",
    "print(\"\\t{:<20}{:.4f}\".format('intercept', logit2_3_intercept))\n",
    "print(\"\\t{:<20}{:.4f}\".format('test', logit2_3_coef_test_score))\n",
    "print(\"\\t{:<20}{:.4f}\".format('avg_ap', logit2_3_coef_avg_ap))\n",
    "\n",
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit2_3' are:\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\"\n",
    "    .format(acc_train_logit2_3, acc_cv_logit2_3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q2.4** Interpret the coefficient estimates for both predictors in `logit2_3` and compare the coefficient estimate for `test_score` to the one from `logit2_1`.  Why has this estimate changed?\n",
    "\n",
    "You should inspect the relationship between `test_score` and `avg_ap` to help get a better sense for what might be happening here.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Print the parameters of the last 2 models\n",
    "print(\"The intercept and coefficient for the 'logit2_1' model are:\")\n",
    "\n",
    "print(\"\\t{:<20}{:.4f}\".format('intercept', logit2_1_intercept))\n",
    "print(\"\\t{:<20}{:.4f}\".format('test_score', logit2_1_coef_test_score))\n",
    "\n",
    "print(\"The intercept and coefficients for the 'logit2_3' model are:\")\n",
    "print(\"\\t{:<20}{:.4f}\".format('intercept', logit2_3_intercept))\n",
    "print(\"\\t{:<20}{:.4f}\".format('test_score', logit2_3_coef_test_score))\n",
    "print(\"\\t{:<20}{:.4f}\".format('avg_ap', logit2_3_coef_avg_ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q2.5** Interpret and compare the train and cv accuracies for the two models, `logit2_1` and `logit2_3`.  Explain why these accuracies are the same or different, and what about the data makes these accuracies so similar or different.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Display accuracies of last 2 models\n",
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit2_1' are:\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\"\n",
    "    .format(acc_train_logit2_1, acc_cv_logit2_1)\n",
    ")\n",
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit2_3' are:\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\"\n",
    "    .format(acc_train_logit2_3, acc_cv_logit2_3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 3: Harvard and Yale?</div>\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q3.1**\n",
    "- Fit a logistic regression model (call it `logit3_1`) to predict `admitted` from 7 predictors: `[\"harvard\", \"test_score\", \"ap\", \"avg_ap\", \"gpa\", \"female\", \"minority\"]`.\n",
    "- Store the train and mean cv accuracies in `acc_train_logit3_1` and `acc_cv_logit3_1`.\n",
    "- Interpret the coefficients for the binary predictors in this model.\n",
    "\n",
    "**Hint:**\n",
    "- If you have convergence warnings, increasing the maximum number of iterations will likely solve this issue.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Print statements below assume predictors used in this order\n",
    "logit3_1_predictors = [\"harvard\", \"test_score\", \"ap\", \"avg_ap\", \"gpa\", \"female\", \"minority\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "logit3_1 = ...\n",
    "acc_train_logit3_1 = ...\n",
    "acc_cv_logit3_1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab model's intercept and coefs regardless of their shape\n",
    "# (which depends on how x and y were shaped)\n",
    "logit3_1_intercept = np.array(logit3_1.intercept_).flatten()[0]\n",
    "logit3_1_coefs = logit3_1.coef_.flatten()\n",
    "\n",
    "print(\n",
    "    \"The intercept and coefficients for the 'logit3_1' model are:\"\n",
    "    \"\\n\\n\\t{:<20}{:.4f}\".format(\n",
    "        \"intercept\", logit3_1_intercept\n",
    "    )\n",
    ")\n",
    "for predictor, coef in zip(logit3_1_predictors, logit3_1_coefs):\n",
    "    print(\"\\t{:<20}{:.4f}\".format(predictor, coef))\n",
    "\n",
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit3_1' are:\"\n",
    "    \"\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\"\n",
    "    .format(acc_train_logit3_1, acc_cv_logit3_1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q3.2** Fit a logistic regression model (call it `logit3_2`) to predict `admitted` from 3 predictors: `[\"harvard\", \"test_score\", \"ap\"]` along with the 2 interaction terms: `harvard` with `test_score` and `harvard` with `ap`. Name the columns for these interaction terms `harvard_test_score` and `harvard_ap`. Store the train and mean cv accuracies in `acc_train_logit3_2` and `acc_cv_logit3_2`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Print statement below assumes this order of predictors\n",
    "logit3_2_predictors = [\"harvard\", \"test_score\", \"ap\", \"harvard_test_score\", \"harvard_ap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "logit3_2 = ...\n",
    "acc_train_logit3_2 = ...\n",
    "acc_cv_logit3_2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print results of logit3_2\n",
    "logit3_2_intercept = np.array(logit3_2.intercept_).flatten()[0]\n",
    "logit3_2_coefs = logit3_2.coef_.flatten()\n",
    "\n",
    "print(\n",
    "    \"The intercept and coefficients for the 'logit3_2' model are:\"\n",
    "    \"\\n\\n\\t{:<20}{:.4f}\".format(\n",
    "        \"intercept\", logit3_2_intercept\n",
    "    )\n",
    ")\n",
    "\n",
    "for predictor, coef in zip(logit3_2_predictors, logit3_2_coefs):\n",
    "    print(\"\\t{:<20}{:.4f}\".format(predictor, coef))\n",
    "    \n",
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit3_2' are:\"\n",
    "    \"\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\"\n",
    "    .format(acc_train_logit3_2, acc_cv_logit3_2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q3.3** Simplify and write out mathematically the above model from Question 3.2 for 2 applicants:\n",
    "1. someone who is applying to Harvard\n",
    "2. someone who is applying to Yale (keep `test_score` and `ap` as the unknown $X$s).\n",
    "\n",
    "The basic framework given to you below may be helpful:\n",
    "\n",
    "$$ \\ln \\left( \\frac{P(Y=1)}{1-P(Y=1)} \\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p $$\n",
    "    \n",
    "**NOTE:** All of your mathematical statements should be written out in your markdown cells using $\\LaTeX$. Show all your steps, not just the final result.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q3.4** Determine two classification boundaries mathematically for the model in the previous part (using the estimated coefficients): What range of values of `test_score` as a function of `ap` would an applicant be predicted to have a better than 50% chance (i.e. 0.5 probability) of being admitted into the college they applied to? \n",
    "\n",
    "Use the function for Harvard to answer the following question: if a student scored a perfect 2400 on `test_score`, what is the range of AP tests they should take in order to have a better than 50% chance of being admitted into Harvard?\n",
    "\n",
    "Again, you should show your work in $\\LaTeX$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q3.5** Create two separate scatterplots (one for Harvard applicants and one for Yale applicants) with the predictor `test_score` on the y-axis and `ap` on the x-axis where `admitted` is color-coded and the marker denotes train vs. test data.  Then add the appropriate classification boundary from the previous question (Q3.4).  Compare these two plots (including both the location of the boundaries and where the points lie around these boundaries).\n",
    "\n",
    "**NOTE:** As always, please be certain (a) your plot is titled, (b) everything is clearly labeled, and (c) the plot itself is formatted in a manner that makes it easy to read and interpret. It will likely take some careful work here to generate plots with data points that are clear and easy to see. You might try 'dithering' the points with a random offset so they are not all on top of one another.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q3.6** Fit a logistic regression model (call it `logit3_6`) to predict `admitted` from 4 predictors: `[\"harvard\", \"test_score\", \"female\", \"minority\"]` along with 2 interaction terms: `harvard` with `female` and `harvard` with `minority`. \n",
    "\n",
    " - Store the train and mean cv accuracies in `acc_train_logit3_6` and `acc_cv_logit3_6`.\n",
    " - Display the accuracy scores.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Provided code in Q3.7 will assume this order of predictors\n",
    "logit3_6_predictors = [\"harvard\",\n",
    "                       \"test_score\",\n",
    "                       \"female\",\n",
    "                       \"minority\",\n",
    "                       \"harvard_female\",\n",
    "                       \"harvard_minority\"\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "logit3_6 = ...\n",
    "acc_train_logit3_6 = ...\n",
    "acc_cv_logit3_6 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit3_6' are:\"\n",
    "    \"\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\"\n",
    "    .format(acc_train_logit3_6, acc_cv_logit3_6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q3.7** Interpret the coefficients associated with `female` and `minority` (the two main effects AND the two interaction terms).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "logit3_6_intercept = np.array(logit3_6.intercept_).flatten()[0]\n",
    "logit3_6_coefs = logit3_6.coef_.flatten()\n",
    "\n",
    "print(\n",
    "    \"The intercept and coefficients for the 'logit3_6' model are:\"\n",
    "    \"\\n\\n\\t{:<20}{:.4f}\".format(\n",
    "        \"intercept\", logit3_6_intercept\n",
    "    )\n",
    ")\n",
    "\n",
    "for predictor, coef in zip(logit3_6_predictors, logit3_6_coefs):\n",
    "    print(\"\\t{:<20}{:.4f}\".format(predictor, coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q3.8** Based on this model, how does it appear that Harvard and Yale compare in admitting women and minorities?  Given what we've seen in our previous analysis, what might be some reasons we have to doubt the interpretation of the model's coefficients as reflecting the truth?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part4\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 4: Building predictive models for admitted</div>\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q4.1** You were instructed to NOT scale predictors in the prior sections above. The primary reason for this was to focus instead on the interpretability of our logistic regression coefficients. However, as we're sure you noticed, the numeric scale among our different predictors varies greatly (i.e. `test_score` values are in the 1,000's while others are much, much smaller). In practice, we might want to put our predictors all on a similar scale, particularly for regularized regression and/or distance-based algorithms such as $k$-NN classification. \n",
    "\n",
    "1. Explain why scaling under these circumstances might be important.\n",
    "2. Define a list of all non-binary predictors from the original set of 12 predictors, calling it `non_binary_predictors`.\n",
    "3. Apply standardized scaling to all of these **non-binary** predictors. **For the sake of consistency, fit your scaler on just the training data. Then use it to transform both train and test.**\n",
    "\n",
    "**IMPORTANT:** These scaled predictors should be used instead of the original unscaled versions of the predictors for the remainder of this problem set. Tests from this point on assume that `X_train` and `X_test` have been standardized with the approach outlined above. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "non_binary_predictors = ...\n",
    "\n",
    "# apply standard scaler to non-binary predictors\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# preview summary stats after standardizing\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q4.2** Fit a well-tuned $k$-NN classification model with main effects of all 12 predictors in it (call it `knn_model`).  Use `ks = range(1, 50, 2)` and 10-fold cross-validation with classification accuracy as the scoring metric. Plot, on a single set of axes, your resulting cross-validation mean training and mean validation scores at each value $k$. Then, store your chosen $k$ in `best_k`, the accuracy on the best model when refit on all the training data in `knn_train_acc`, and the mean 10-fold CV accuracy of the best model in `knn_cv_acc`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ks = range(1, 50, 2)\n",
    "\n",
    "# your code here\n",
    "...\n",
    "best_k = ...\n",
    "...\n",
    "knn_train_acc = ...\n",
    "knn_cv_acc = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot cross-validation results\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The classification accuracies for 'knn_model' where k={} \"\n",
    "    \"are:\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\"\n",
    "    .format(\n",
    "        best_k, knn_train_acc, knn_cv_acc\n",
    "    )\n",
    ")\n",
    "# create dict for storing test scores for each Q4 model\n",
    "q4_cv_scores = {\"knn_model\": knn_cv_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q4.3** Fit the full logistic regression model (without penalty) with main effects of all 12 predictors in it (call it `logit_full`). Store the train mean cv accuracy in `logit_full_train_acc` and `logit_full_cv_acc`.\n",
    "\n",
    "**HINT:** If you have convergence warnings, increasing the maximum number of iterations will likely solve this issue.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# The code that prints your results assumes\n",
    "# predictors were given to the model in this order\n",
    "predictor_list = [\n",
    "    \"test_score\", \"ap\", \"avg_ap\", \"sat_subjects\",\n",
    "    \"gpa\", \"female\", \"minority\", \"international\",\n",
    "    \"sports\", \"harvard\", \"early_app\", \"alumni\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "logit_full = ...\n",
    "\n",
    "logit_full_train_acc = ...\n",
    "logit_full_cv_acc = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display your results\n",
    "print(\n",
    "    \"The intercept and coefficients for the 'logit_full' model are:\"\n",
    "    \"\\n\\n\\t{:<20}{:.4f}\".format(\n",
    "        \"intercept\", np.array(logit_full.intercept_).flatten()[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "for predictor, coef in zip(predictor_list, logit_full.coef_.flatten()):\n",
    "    print(\"\\t{:<20}{:.4f}\".format(predictor, coef))\n",
    "\n",
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit_full' are:\"\n",
    "    \"\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\"\n",
    "    .format(logit_full_train_acc, logit_full_cv_acc)\n",
    ")\n",
    "\n",
    "# store test score to dict for later use\n",
    "q4_cv_scores[\"logit_full\"] = logit_full_cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q4.4** Fit a well-tuned Lasso-like logistic regression model from all 12 predictors in it (call it `logit_lasso`). Use `Cs = np.logspace(-2, 2, 50)` and 10-fold cross-validation. Store the train and mean cross-validation accuracies in `logit_lasso_train_acc` and `logit_lasso_cv_acc`. For tuning a regularized logistic regression model, you should use `LogisticRegressionCV`. But again, to insure reproducability here, you should set `random_state=109`. \n",
    "\n",
    "**Hint:**\n",
    "- The default solver for SKLearn's logistic regression, 'lbfgs', is not compatible with LASSO regularization, so you will need to use a different solver here. See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) for guidance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2, 2, 50)\n",
    "# your code here\n",
    "...\n",
    "logit_lasso = ...\n",
    "logit_lasso_train_acc = ...\n",
    "logit_lasso_cv_acc = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display your results\n",
    "print(\n",
    "    \"The intercept and coefficients for the 'logit_lasso' model are:\"\n",
    "    \"\\n\\n\\t{:<20}{:.4f}\".format(\n",
    "        \"intercept\", np.array(logit_lasso.intercept_).flatten()[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Again, this code assumes predictors were given to the model\n",
    "# in the order defined in `predictor_list`\n",
    "for predictor, coef in zip(predictor_list, logit_lasso.coef_.flatten()):\n",
    "    print(\"\\t{:<20}{:.4f}\".format(predictor, coef))\n",
    "\n",
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit_lasso' are:\"\n",
    "    \"\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\"\n",
    "    .format(logit_lasso_train_acc, logit_lasso_cv_acc)\n",
    ")\n",
    "\n",
    "# store cv score to dict for later use\n",
    "q4_cv_scores[\"logit_lasso\"] = logit_lasso_cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.5** Which predictors were deemed important in `logit_lasso`?  Which were deemed unimportant? Here we assume that any predictors with zero-valued Lasso coefficients \"unimportant\". Store your results in `predictors_important` and `predictors_not_important`.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "predictors_important = ...\n",
    "predictors_not_important = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display results\n",
    "print(\n",
    "    \"The following predictors were deemed important by \"\n",
    "    \"'logit_lasso' (i.e. coef != 0):\\n\\n\\t{}\\n\\n\\n\"\n",
    "    \"While, the remaining were deemed unimportant (i.e. \"\n",
    "    \"coef == 0):\\n\\n\\t{}\"\n",
    "    .format(\n",
    "        predictors_important,\n",
    "        predictors_not_important,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q4.6** Fit a well-tuned Lasso-like logistic regression model with all important predictors from `logit_lasso` and all the unique 2-way interactions between them (call it `lasso_interact`).  Again use `Cs = np.logspace(-2, 2, 50)`,  and 10-fold cross-validation with classification accuracy as the scoring metric. Record the accuracy on train and test for this model in `lasso_interact_train_acc` and `lasso_interact_test_acc`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2, 2, 50)\n",
    "# your code here\n",
    "...\n",
    "lasso_interact = ...\n",
    "\n",
    "...\n",
    "lasso_interact_train_acc = ...\n",
    "lasso_interact_cv_acc = ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\\nThe classification accuracies for 'logit_lasso_interact' are:\"\n",
    "    \"\\n\\n\\tTrain\\t{:.4f}\\n\\tCV\\t{:.4f}\\n\"\n",
    "    .format(lasso_interact_train_acc, lasso_interact_cv_acc)\n",
    ")\n",
    "\n",
    "# store test score to dict for later use\n",
    "q4_cv_scores[\"lasso_interact\"] = lasso_interact_cv_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q4.7** How many of the predictors in our `logit_lasso_interact` model were deemed important and unimportant? Store these numbers in `num_important_coefs` and `num_unimportant_coefs`. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "num_important_coefs = ...\n",
    "num_unimportant_coefs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Of the {} predictors used in our 'logit_lasso_interact' model:\"\n",
    "    \"\\n\\n\\t{} predictors were deemed 'important' by our model\"\n",
    "    \"\\n\\t{} predictors were deemed 'unimportant' with 0-valued \"\n",
    "    \"coefficients\".format(\n",
    "        num_important_coefs + num_unimportant_coefs,\n",
    "        num_important_coefs,\n",
    "        num_unimportant_coefs\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part5\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 5: Evaluating results</div>\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q5.1** Which of the 4 models in Question 4 performs the best based on cross-validation accuracy?  Which performs the worst? Based on these accuracies, how do these models perform compared to your baseline \"naive\" model's performance on the training data back in Question 1.1? What does this comparison to the \"naive\" model tell us?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "print(\"The CV accuracies for the 4 models in Q4 are:\\n\")\n",
    "for key, value in q4_cv_scores.items():\n",
    "    print(\"\\t{:<25}{:.4f}\".format(key, value))\n",
    "print(\n",
    "    \"\\nThe  accuracy for our original baseline \\\"naive\\\" \"\n",
    "    \"model was {:.4f}\".format(naive_train_acc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q5.2** Draw an ROC curve for each of the four models on a single plot.  How do these ROC curves compare?  Do the ROC curves support that the best model identified in Question 5.1 is better than the worst model identified in 5.1?  How do you know?\n",
    "\n",
    "**Hint:** Use `cross_val_predict` with `method='predict_proba` to get the probability predictions required to construct the ROC curves.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**5.3** Calculate the AUC for all 4 models and store these in `auc_list`. **The order of AUCs in the list should match the order of the model as they appear in `q4_cv_scores`.**\n",
    "    \n",
    "Do the rankings of these 4 models based on AUC match those for classification accuracy?  Why do you think this is the case?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# calculate each model's AUC using its ROC fpr and tpr\n",
    "auc_list = ...\n",
    "    \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The CV accuracies & CV AUC scores for the 4 models in Q4 are:\"\n",
    "    \"\\n\\n\\t\\t\\t\\tAccuracy\\tAUC\"\n",
    ")\n",
    "for (key, value), auc_value in zip(q4_cv_scores.items(), auc_list):\n",
    "    print(\"\\t{:<24}{:.4f}\\t\\t{:.4f}\".format(key, value, auc_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**Q5.4** Select your preferred model based on the cross-validation accuracy and AUC. For this 'final model', display the accuracy and AUC scores on the **test data**. Compare these to the accuracy and AUC score of the naive model from 1.1 when predicting on the test data. Would you offer your final model as a publicly available tool for college applicants to use to determine their chances of getting into Harvard and/or Yale? Why or why not? What might be some consequences of the deployment of the model into the real world?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<a id=\"part6\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 6: BONUS!</div>\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q6 (optional)** Incorporate what you've learned from the PCA and/or Missingness sections of the course to create a new, improved classification model. You could focus on improving predictive performance, inference, interpretability, or all of the above! \n",
    "\n",
    "Be sure to also describe your approach and analyze your results.\n",
    "\n",
    "While this section is optional, we highly encourge you to experiment (and not just for the sake of the potential bonus points 😉).\n",
    "\n",
    "**Note:** Though you should report the test performance of this new, \"bonus\" model, selecting the bonus model over the 'final model' from Q5.4 would have to be justified based on cross-validation performance, *not* test performance! \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"highlight-box\" style=\"border: 1px solid #ffcccb; background-color: #ffcccb; height: 5px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color: #4a4a4a; background-color: #fbe8ff; border-color: #eed4db; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "**Wrap-up**\n",
    "\n",
    "* Please describe the aspect(s) of the assignment you found most challenging. This could be conceptual and/or related to coding and implementation. This is also a good place to ask any remaining questions you may have.\n",
    "\n",
    "* How many hours did you spend working on this assignment? Store this as an int or float in `hours_spent_on_hw`. If you worked on the project in a group, report the *average* time spent per person.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hours_spent_on_hw = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"wrapup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_end = time.time()\n",
    "print(f\"It took {(notebook_end - notebook_start)/60:.2f} minutes for this notebook to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🌈 **This concludes HW5. Thank you!**\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 2.5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(naive_train_acc, 0.7299, atol=0.0001), 'Incorrect naive train accuracy'\n",
         "hidden": false,
         "locked": false,
         "points": 2.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.4": {
     "name": "q1.4",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert type(logit1_4) == type(LogisticRegression()), 'logit1_4 should be am sklearn LogisticRegression object'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> assert np.isclose(p_harvard, 0.2938, atol=0.0001), 'Incorrect value for p_harvard'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert np.isclose(p_yale, 0.2445, atol=0.0001), 'Incorrect value for p_yale'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(logit2_1_intercept, -10.5474, atol=0.0001), 'logit2_1_intercept should be a float between -10 and 10'\n>>> assert np.isclose(logit2_1_coef_test_score, 0.0043, atol=0.0001), 'logit2_1_coef_test_score should be a float between -1 and 1'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert np.isclose(acc_train_logit2_1, 0.7299, atol=0.0001), 'acc_train_logit2_1 is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert np.isclose(acc_cv_logit2_1, 0.73, atol=0.0001), 'acc_cv_logit2_1 is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(prob_test_2250, 0.3019, atol=0.0001), 'prob_test_2250 is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1.25
        },
        {
         "code": ">>> assert np.isclose(prob_test_2400, 0.4525, atol=0.0001), 'prob_test_2400 is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1.25
        },
        {
         "code": ">>> assert np.isclose(test_50_50, 2444.2, atol=0.01), 'test_50_50 is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(logit2_3_intercept, -10.9086, atol=0.0001), 'logit2_3_intercept is incorrect.'\n>>> assert np.isclose(logit2_3_coef_test_score, 0.0046, atol=0.0001), 'logit2_3_coef_test is incorrect.'\n>>> assert np.isclose(logit2_3_coef_avg_ap, -0.0723, atol=0.0001), 'logit2_3_coef_avg_ap is incorrect.'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert np.isclose(acc_train_logit2_3, 0.7306, atol=0.0001), 'acc_train_logit2_3 is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert np.isclose(acc_cv_logit2_3, 0.7299, atol=0.0001), 'acc_av_logit2_3 is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(acc_train_logit3_1, 0.7353, atol=0.0001), 'acc_train_logit3_1 is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert np.isclose(acc_cv_logit3_1, 0.73, atol=0.0001), 'acc_test_logit3_1 is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.2": {
     "name": "q3.2",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(logit3_2_coefs) == 5, 'logit3_2 should have 5 coefficients (not including the intercept)'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert np.isclose(acc_train_logit3_2, 0.736, atol=0.0001), 'logit3_2 coef values is incorrect'\n>>> assert np.isclose(acc_cv_logit3_2, 0.7353, atol=0.0001), 'logit3_2 coef values is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.6": {
     "name": "q3.6",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(logit3_6.coef_.flatten()) == 6, 'logit3_6 should have 6 coefficients (not including the intercept)'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert np.isclose(acc_train_logit3_6, 0.7353, atol=0.0001), 'logit3_6 train accuracy is incorrect.'\n>>> assert np.isclose(acc_cv_logit3_6, 0.7279, atol=0.0001), 'logit3_6 CV accuracy is incorrect.'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.1": {
     "name": "q4.1",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(X_train[non_binary_predictors].values.mean(), 0), 'Non-binary predictors in train should now have mean 0'\n>>> assert np.isclose(X_test[non_binary_predictors].values.mean(), 0, atol=0.1), 'Non-binary predictors in test should now have mean near 0'\n>>> assert np.isclose(X_train[non_binary_predictors].values.std(), 1), 'Non-binary predictors in train should now have std 1'\n>>> assert np.isclose(X_test[non_binary_predictors].values.std(), 1, atol=0.1), 'Non-binary predictors in test should now have std near 1'\n>>> assert set(X_train['harvard'].unique()) == {0, 1}, 'You should not scale the binary predictors.'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.3": {
     "name": "q4.3",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(logit_full_cv_acc, 0.738, atol=0.001), 'logit_full_cv_acc is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 1.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.4": {
     "name": "q4.4",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(logit_lasso_train_acc, 0.742, atol=0.002), 'logit_lasso_train_acc is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert np.isclose(logit_lasso_cv_acc, 0.738, atol=0.002), 'logit_lasso_cv_acc is incorrect'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.5": {
     "name": "q4.5",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(predictors_important) == 11, 'Incorrect number of important predictors'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert len(predictors_not_important) == 1, 'Incorrect number of non-important predictors'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.7": {
     "name": "q4.7",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert num_important_coefs == 44, 'Incorrect number of important predictors'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert num_unimportant_coefs == 22, 'Incorrect number of non-important predictors'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5.3": {
     "name": "q5.3",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(auc_list) == 4, 'You should have 4 entries in auc_list'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> assert np.allclose(auc_list, [0.6528, 0.7013, 0.7024, 0.6946], atol=0.001), 'AUC scores are off.'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "wrapup": {
     "name": "wrapup",
     "points": 0,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert float(hours_spent_on_hw), 'Please select a time in hours (int or float) to specify how long you spent on this assignment.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
